
# Language Models are Few-Shot Learners

<br/>

## Abstract

최근 자연어 처리 분야에서의 연구들은 방대한 텍스트 말뭉치(corpus)를 바탕으로 사전 학습한 후, 특정 태스크의 목적에 맞게 미세 조정하는 방법을 통해 많은 성과를 거두었다. 이러한 방식은 그 모델의 아키텍쳐 측면에서는 태스크에 민감하지 않지만(task agnostic), 학습 방법에 있어서는 여전히 수천, 수만의 예시 데이터를 통해 특정 태스크에 특화된(task specific) 미세 조정을 필요로한다. 이는 사람이 일반적으로 몇 개의 예시나 굉장히 간단한 지시 사항만으로도 손쉽게 새로운 자연어 처리 태스크를 수행할 수 있는 것과는 대비된다.

본 연구는 **언어 모델의 스케일을 키우는 것**이 태스크에 대한 일반화 가능성(task agnostic)과 **few shot** 성능을 높이며, 미세 조정을 활용하는 이전의 sota모델들과도 비등한 성능을 확보할 수 있다는 것을 보인다. 기존의 non-sparse 언어 모델보다 10배 많은 1750억개의 변수를 가지는 auto regressive 언어 모델 GPT-3를 학습시켜, few shot 세팅에서의 성능을 측정했다.

모든 태스크에 대해 GPT-3는 그 어떤 gradient 갱신, 미세 조정도 거치지 않으며 오직 few shot 세팅을 취한다. 해당 모델은 번역, 질의 응답, 번역, 3자리 연산, domain adaptatin, 새로운 단어를 쓰는 능력 등 다양한 자연어 처리 태스크에서 높은 성능을 보였다. 또한 GPT-3가 few shot 학습을 수행할 때 여전히 잘 해결하지 못하는 몇 가지 데이터셋을 확인했다. (ex) 대규모의 web corpora 데이터)  또한 GPT-3가 사람이 판별하기 어려운 수준의 가짜 뉴스를 작성할 수 있고, GPT-3가 어떠한 사회적 영향력을 가질 수 있는 지 논의한다.

<br/>

## 1. Introduction

최근 몇 년간 자연어 처리 시스템에서 사전학습된 언어 표현을 사용하는 추세가 대두되었으며, downstrea task에 대한 전이를 위한 유연하고 **task-agnositc**한 방식으로 적용되었다. 단어 벡터들을 사용하여 단일 레이어 표현을 학습하고, 학습한 표현을 task specific한 모델 구조에 입력으로 넣은 후, 보다 강력하고 유용한 표현을 얻기 위해 다중 레이어 RNN과 contextual state가 활용되었다. 최근에는 사전 학습된 transformer 언어 모델이 직접 미세 조정하는 방식을 통해 task specific한 모델 구조의 필요성을 없앴다. 

이와 같은 패러다임(transformer 기반)이 자연어 처리 분야에 혁신을 가져온 것은 사실이지만, 모델 구조가 task agnostic하더라도 여전히 task specific한 데이터와 미세 조정 단계를 필요로 한다는 것이다. 특정 태스크 수행을 위한 강력한 성능을 위해서는 해당 태스크에 초점을 맞춘 수천~수만개의 데이터에 대한 미세 조정이 필요하다. 즉, **태스크와 무관하게 학습된 모델이더라도, 좋은 성능을 위해서는 대규모의 데이터를 바탕으로한 미세 조정이 필요하다는 한계가 있다.** 이러한 한계를 극복하는 것은 다음과 같은 관점에서 가치가 있다.

  1. 현실적으로 새로운 태스크마다 레이블링이 전부 되어 있는 대용량의 데이터셋을 필요로 하는 것은 언어 모델의 활용 가능성을 크게 제한한다. 광범위한 분야에서 유용한 자연어 처리 태스크들이 존재하는데, 이러한 태스크들에 각각 적합한 **대용량의 지도학습용 데이터셋을 구축하는 것은 매우 어려운 일**.
  
  2. 모델의 표현력과 학습 데이터 분포의 narrowness에따라 학습 데이터에 존재하는 spurious correlation(왜곡된 상관관계)을 활용한 가능성이 크게 증가한다. 이는 모델이 사전 학습을 통해 대량의 정보를 학습하지만, 굉장히 작은 태스크 데이터 분포에 국한되어 미세 조정된다는 점에서 기존의 사전 학습 + 미세 조정의 패러다임에 의문을 제기한다. 즉, **훈련 데이터의 분포에 대해 한정지어진 모델이 그 외의 영역은 잘 일반화하지 못하는 out of distribution문제**가 발생할 수 있다.
  
  3. 인간은 새로운 언어 태스크를 위해 대규모의 지도학습용 데이터(examples)를 필요로 하지 않는다. **간단한 지시문이나 적은 수의 예시만 있어도 충분히 해당 태스크를 잘 수행할 수 있다.**

위와 같은 문제들을 해결할 수 있는 방법들 중 가능성이 높은 것은 언어 모델의 문맥에서 모델이 학습하는 동안 여러 기술과 패턴 인식 능력을 학습하고, 추론에는 이러한 능력을 원하는 태스크에 빠르게 적용시킬 수 있는 방법인 Meta-learning이다. **In context learning** 단계에서 사전 학습된 언어 모델의 텍스트 입력을 task specification 형태로 사용하며, 이 모델은 지시문과 task 설명을 바탕으로 다음에 무엇이 올 것인지를 예측한다. (몇 가지 예제나 태스크에 대한 설명을 제공)

![image](https://user-images.githubusercontent.com/44194558/136746301-3270812d-28d7-4f2e-8c37-eaf55f505c6d.png)

하지만 위의 방법은 기존의 미세조정 방식과 비교하면 성능이 떨어지며, Meta learning으로 자연어 처리 태스크를 해결하는 방식은 현실적으로 많은 개선 사항을 필요로하는 한계가 있다.

언어 모델의 또 다른 최신 경향은 **모델의 크기**를 키우는 것이다. 최근 transformer 언어 모델의 파라미터 수는 1억개에서 170억개 까지 늘어났고, 모델의 스케일이 커질 수록 다양한 자연어 처리 태스크에서 상당한 수준의 성능 개선을 보였다. 즉 많은 downstream task에 대한 log loss는 모델의 스케일이 커짐에 따라 개선되는 경향이 뚜렷하게 나타난다. In context learning이 모델의 파라미터를 통해 많은 스킬과 태스크를 학습하기 때문에, 해당 능력은 모델의 스케일이 증가함에 따라 향상된다고 보는 것이 합당하다. 

본 연구는 **1750억 개의 파라미터를 가지는 auto regressive 언어 모델인 GPT-3를 학습**함으로서 위의 가설을 검증하고, 문맥 내 학습 능력을 측정한다. 구체적으로 학습 데이터셋에 포함되어 있지 앟은 태스크들에 빠르게 적용될 수 있는 지를 테스트할 수 있도록 20개 이상의 자연어 처리 데이터에 대한 평가를 진행했다. 개별 태스크에 대핸 3가지 조건에서 GPT를 평가한다.

  1. Few shot learning(In context Learning) : 모델의 문맥 윈도우(10~100)에 적합한 설명이나 예시들을 허용 
  
  2. One shot learning : 하나의 예시만 허용
  
  3. Zero shot learning : 예제 사용 없이 태스크에 대한 설명이나 지시 사항만 전달

다음은 모델이 단어와 관련 없는 기호를 제거하는 태스크에 대한 few shot learning의 결과

![image](https://user-images.githubusercontent.com/44194558/136749301-4c06476e-cea6-41f0-b6a8-f89dd99fd525.png)

  * 성능을 측정하는 동안에는 **gradient 업데이트나 미세 조정은 이루어지지 않음.**
  
  * 태스크에 대한 자연어 설명(prompt)가 모델 성능을 향상시킴
  
  * 모델의 문맥 위도우에 더 많은 예제(K)가 있을 수록 성능 향상
  
  * 큰 모델일 수록 문맥 내 학습에서 월등한 성능을 보임 

종합적으로 GPT-3는 zero shot, one shot에서도 높은 성능을 보이고, few shot에서는 미세 조정 기법을 사용한 모델에 비해서도 sota 성능을 자랑한다. GPT-3의 스케일에도 불구하고 해당 모델이 어려움을 겪는 몇 가지 태스크를 확인 했다. 이러한 한계를 포함하여 GPT-3의 장단점을 보이고, few shot learning의 발전을 위해 한계점을 분석한다.

또한 본 연구는 **데이터 오염**(학습, 테스트 데이터셋이 겹치는 문제)에 대해서도 분석을 진행했다. Common crawl같은 데이터를 바탕으로 스케일이 큰 모델을 학습시킬 때 발생할 수 있는 문제로 학습, 테스트 데이터가 우연히 겹치는 부분이 적지 않을 수 있다. 본 연구는 데이터 오염과 왜곡 효과를 측정할 수 있는 체계적 방법을 제안한다. 그리고 GPT-3의 성능은 대부분의 데이터셋에서 데이터 오염의 왜곡 효과가 미미한 수준이었지만, 약간의 데이터셋의 오염이 충분히 큰 왜곡을 가져올 수 있음을 보이고, 오염의 심각도가 큰 데이터셋에 대해서는 *표시를 하여 결과에 포함시키지 않았다.

최종적으로 본 연구는 GPT-3가 보이는 높은 성능에서 **편향성, 공정성, 사회적 영향력**과 같은 모델의 특성 역시 분석하였다.

<br/>

## 2. Approach

기본적인 사전 학습은 GPT-2와 매우 유사하며 모델 크기를 키우고, 데이터의 양과 다양성을 증가시키며 훈련의 기간을 늘렸다. 문맥 내 학습 역시 GPT-2와 유사하지만 체계적으로 세팅을 다르게 하였기 때문에, 여러 세팅들을 정의하고 그 결과를 비교한다. 세팅은 task specific한 데이터에 얼마나 의존하는 경향이 있는 지를 나타낸다.

<br/>

### Fine-Tuning

최근 들어 가장 일반적으로 사용되는 방법으로, 사전 학습된 모델의 가중치를 원하는 태스크에 맞도록 지도학습 데이터셋을 바탕으로 업데이트(학습)하는 방식이다. 수천~수만 개의 레이블링된 예제들을 필요로한다. 이러한 방법은 성능 향상에 크게 기여해왔으나, 모든 태스크들에 적합한 대규모의 데이터셋을 필요로하고, 분포 외의 데이터(Out Of Distribution)에 대해서는 일반화 성능이 떨어지고, 학습 데이터에 존재하는 거짓/비논리적인 특성(spurious feature)을 학습하여 사람에 비해 불공정한 비교 결과로 이어질 수 있다. **본 연구의 목적은 task agnosic performance이기 때문에 GPT-3에 대한 미세 조정을 수행하지 않는다.** 하지만 유의미한 future work가 될 수는 있음.

<br/>

### Few-Shot

본 연구에서 사용되는 방법으로 **모델이 추론 시간에 몇 개의 예시들을 보게 되지만, 가중치에 대한 갱신은 허용되지 않는다.** 일반적인 데이터셋의 예시는 영어-> 독일어 번역 처럼 문맥과 원하는 정답이 존재하고 few shot은 K개의 예시와 정답이 주어진다. 주어진 K(10~100)개의 예제를 문맥(2048 토큰까지 처리)에 주고, 추론하려는 예시의 결과를 완성하도록 하는 접근법이다. 최종적으로 한 개의 문맥이 주어지면 모델은 정확한 결과를 출력해야한다.

  >EX) '한국어를 영어로 번역하세요' : 집에 갈래요 -> I want to go home, 배고파 -> I am hungry, ...

Few shot은 task specific한 데이터가 몇 개 없어도 충분하며, 지나치게 좁은 분포를 갖는 미세 조정용 데이터셋에 대한 학습 가능성을 줄이지만, 미세 조정 모델의 sota에 비해서 한참 뒤떨어 지는 성능을 보이고, 아무리 적은 수라고 해도 결국 여전히 task specific한 데이터를 필요로한다.

<br/>

### One-shot

Few shot과 비슷하나 **단 한 개의 예시와, 태스크에 대한 자연어 지시문**이 제공된다. 이 방법은 사람이 의사소통하는 방법과 가장 유사하다는 점에서 few, zero shot방법과 큰 차이가 있다. 예를 들어, 사람에게 데이터셋을 만들어내라는 요청을 할 때 지시문과 함께 주로 태스크에 대한 하나의 예시만으로도 충분하다. 예시가 아예 없다면 태스크의 내용과 형식에 대한 의사소통이 조금 힘들 수는 있다.

<br/>

### Zero-shot

One shot과 유사하나, **단 하나의 예시도 없고 모델은 단지 태스크에 대한 지시문만을 받는다.** 거짓 상관관계를 고려할 필요가 없고, 강인하기 때문에 최대의 편의성을 가지나 가장 학습하기 어려운 조건이기도 하다. 사람도 예시가 아예 주어지지 않는 경우 태스크를 제대로 이해하지 못할 수 있다는 점을 고려하면 이 조건은 unfairly hard하다. 그럼에도 불구하고 zero shot은 사람이 태스크를 수행하는 것과 가장 비슷한 방식으로, 대부분의 사람은 텍스트 지시문만을 보고도 어떻게 태스크를 수행해야 하는 지 알 수 있다.

![image](https://user-images.githubusercontent.com/44194558/136774501-1c75b26d-cc17-4a5a-9a28-f87cd5b72910.png)

  * Few shot의 결과는 미세 조정 모델보다 약간 못한 정도의 결과를 보였다
  * One, zero 는 사람이 태스크를 수행하는 것과 아주 유사하나 추후의 연구로 남겨둠

<br/>

### 2.1 Model and Architectures

초기화, 사전 정규화, 토큰화(reversible)등을 비롯한 구조는 GPT-2와 동일한 구조를 갖지만, **transformer 레이어에서 dense and locally banded sparse attention을 번갈아 가며 사용**했다. (sparse attention과 유사함)

![image](https://user-images.githubusercontent.com/44194558/136776926-a005cd89-9864-4325-a2b0-6d56cff8fc10.png)

  * memory complexity를 낮추려는 시도
  * 행 : output / 열 : input
  * 각 output은 제한 개수의 input token에 대해서만 attention score를 볼 수 있음

참고 : https://judy-son.tistory.com/5

모델 크기에 따른 머신러닝 성능을 비교하기 위해 1.25억개 부터 1750억 개(GPT-3)의 파라미터를 가지는 8가지 다른 크기의 모델을 학습시킴.

![image](https://user-images.githubusercontent.com/44194558/136777671-22dc0170-a861-45b3-9e6b-0cdfe7c8e3c3.png)
  
  * GPT-3는 96개의 레이어, 12288차원의 은닉층 차원, 96개의 attention head를 가지는 총 1750억 개의 파라미터 모델
  * 모든 모델은 3000억 토큰에 대해 학습함
  * 큰 모델의 경우 메모리 문제로 인해 행렬 곱셈에 있어 모델 병렬화, 레이어 간의 모델 병렬화를 섞어서 사용

<br/>

### 2.2 Training Dataset

언어 모델을 위한 데이터셋은 빠르게 확장되고 있고, 본 연구는 그 중 거의 1조 개의 단어로 구성된 Common Crawl 데이터셋을 활용한다. 해당 데이터셋은 가장 큰 모델인 GPT-3를 학습 시키기에도 충분함. 하지만 필터링을 전혀 거치지 않은 데이터이기 때문에 조정된 데이터셋에 비해 낮은 품질을 갖는 경향이 있기 때문에 다음과 같은 방법들을 활용하여 품질을 개선함

  > 1. 고품질의 출처와 유사성이 있는 데이터만을 다운로드 받고 정제함
  > 2. 중복 제거 작업 수행
  > 3. 고품질의 출처로 알려진 말뭉치를 추가하여 다양성을 증가

다음은 혼합된 형태의 최종 데이터셋 구성.

![image](https://user-images.githubusercontent.com/44194558/136780422-683634a1-5b69-4ab6-9c24-acfb4c975eeb.png)

   * Common Crawl의 경우 45TB의 데이터셋을 정제하여 570GB로 만듬 (4천억 개의 BPE 토큰으로 구성)
   * 학습에서 데이터의 사용은 데이터셋의 크기에 비례하지 않고 고품질일수록 많이 선택됨 (고품질과 과적합 사이의 trade off)

사전 학습된 언어 모델에서 인터넷에서 가져온 데이터를 사용할 때의 가장 큰 방법론적 문제는 모델이 방대한 양의 내용을 학습하고 기억하는 과정에서 사전 학습동안 무심코 본 정보를 테스트셋에서 다시 마주치는 데이터 오염 문제이다. 이러한 현상을 방지하기 위해 본 연구는 중복을 제거하려는 노력을 기울였다. 하지만 겹치는 부분을 무시하는 버그가 일부 발생했고, 학습의 비용 문제로 인해 다시 훈련하는 것은 현실적으로 어려운 문제였다. 추후에 데이터 오염을 보다 공격적으로 제거하는 연구를 진행할 계획.

<br/>

### 2.3 Training Process

일반적으로 큰 모델일수록 더 큰 batch를 사용하고, 학습률은 더 작게 해야한다. 학습하는 동안 gradient noise scale을 측정하고 이를 batch size 선택의 기준으로 설정함. 메모리 부족 없이 더 큰 모델을 학습시키기 위해 행렬 곱셈에 있어 병렬화를 적용함. 

![image](https://user-images.githubusercontent.com/44194558/136781646-c3776e3a-244e-4008-8b99-39259e72c4bb.png)

<br/>

## 3. Results

### 3.1 Language Modeling, Cloze, and Completion Tasks

Language Modeling
![image](https://user-images.githubusercontent.com/44194558/136782445-cb5cbfe1-b12c-44ae-92a9-27444e8565ba.png)

![image](https://user-images.githubusercontent.com/44194558/136782296-e6553fa1-105d-4c7a-bae7-fe9d5cf93f09.png)

Clozed Book Question Answering

![image](https://user-images.githubusercontent.com/44194558/136782501-22c4f26a-3d4d-4684-9598-bc5f72f41722.png)

Machine Translation

![image](https://user-images.githubusercontent.com/44194558/136782622-9349384c-7f56-4953-a460-155e75324ab2.png)

News Article Generation

![image](https://user-images.githubusercontent.com/44194558/136782661-45230d40-fc5e-4881-91ce-7f278f12422d.png)

<br/>

## GPT-3 : Overfitting or Generalization?

GPT-3를 학습한 사전 학습 데이터는 인터넷에서 얻은 대규모의 데이터이기 때문에 테스트셋의 예제를 이미 봤을 가능성이 존재한다. 이와 같이 넷 스케일의 데이터를 사용함에 있어 발생할 수 있는 데이터 오염 문제는 sota 성능을 달성하는 것 이외의 중요한 연구분야. GPT-2 역시 테스트 데이터가 사전 학습 데이터에 섞여 있을 가능성에 대한 연구를 수행했고, 학습과 테스트 데이터 간의 중복이 있을 때 모델의 성능이 조금 더 좋기는 했으나, 아주 적은 비율의 오염 데이터로 인해 크게 성능이 좌우되지 않는 다는 것을 확인했다.

GPT-3의 경우 GPT-2보다 몇 배는 많은 양의 데이터를 사용하기 때문에 데이터 오염과 테스트셋 암기의 위험성이 더 높다고 볼 수 있다. 그러나 이와 동시에 **데이터 양이 굉장히 방대하기 때문에 GPT-3의 175B 모델 조차 훈련 데이터셋에 대한 과적합이 발생하지 않았다.** 따라서 본 연구는 넷 스케일의 사전 학습 데이터를 사용함에 따라 데이터 오염의 결과가 크지 않을 것이라고 예상한다. 

사전 학습 과정에서 테스트 데이터가 유출된 것의 영향을 평가하기 위해 각 태스크에 대해 사전 학습 데이터와 13-gram으로 중복되는 데이터를 삭제하는 clean version의 테스트셋을 만들어 모델을 보수적으로 평가함. 그 결과 GPT-3가 유출된 데이터에 대해 모델이 더 높은 성능을 보였다는 특별한 증거는 없었다. 오염 비율이 높은 데이터셋에 대해서도 그것이 성능에 미치는 영향은 0에 가까웠다.

![image](https://user-images.githubusercontent.com/44194558/136783926-5d68ce50-c813-49f4-bb0c-582335e8ee45.png)

![image](https://user-images.githubusercontent.com/44194558/136784011-d15f56e8-3e6b-41af-b356-159c090bd08a.png)

<br/>

## Limitations

1. 생성 태스크에 있어 동어 반복 등의 문제가 발생. 문맥 내 학습 능력이 몇몇 태스크에서 성능이 떨어짐.(zero, one shot으로 바꿔도 개선 x)
   
2. 모델의 구조 및 알고리즘 적 한계 : 본 연구는 **auto regressive** 언어 모델의 문맥 내 학습만 고려. 양방향 맥락을 고려하지 않아 빈칸 채우기, 두 문단을 비교하고 답하는 태스크 등에서 한계가 있었음 -> 추후 연구는 GPT-3 스케일에서의 양방향 모델


3. 언어 모델의 스케일을 키우는 것은 사전 학습 목적 함수의 한계에 다다르게 됨. 현재의 목적 함수는 모든 토큰에 대해 동일한 가중치를 적용하고 있고, 어떤 토큰을 예측하는 것이 더 중요한지를 반영하지 못함. 지도학슴 목적 함수를 사용하는 것은 일단 모델이 토큰을 잘 예측할 것을 강요하지만, 궁극적으로는 언어 모델은 '목적 지향성'을 가지기 때문에 단순 예측을 넘어서야 함.


4. 테스트 시에는 사람과 같이 몇 개의 예제나 지시사항 만으로도 문제를 해결할 수 있지만, 사전 학습을 위해서는 인간이 평생 봐도 다 보지 못할 양의 데이터를 봐야 함.


5. Few shot 세팅이 정말로 추론시에 새로운 태스크를 새롭게 배우는 것인지, 훈련하는 동안 배운 태스크 중 하나를 인지해 수행해 내는 것인지는 모호함. (번역 과제는 사전 학습 중 이미 배웠을 수도 있음)


6. 해석 가능성과 설명력이 떨어지고, 데이터에 존재하는 편향(bias)은 편견이 섞인 내용을 생성할 위험이 있음

<br/>

## Broader Impacts

1. 악용될 가능성 (스팸, 피싱, 가짜 뉴스, 잘못된 정보의 전달)


2. 스테레오타입이나 편견이 있는 텍스트를 생성할 가능성

![image](https://user-images.githubusercontent.com/44194558/136785133-dfa2d2fb-d453-4de3-9b50-71122ad9a314.png)

![image](https://user-images.githubusercontent.com/44194558/136785170-7935c37c-1872-4637-b4b5-7ec8f512ca22.png)

![image](https://user-images.githubusercontent.com/44194558/136785221-57add605-9172-4197-963a-f33f22d31ca4.png)